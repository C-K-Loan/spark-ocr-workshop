{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of usage Spark OCR with Image Brands Extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Spark OCR transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install spark-ocr python packge\n",
    "Need specify path to `spark-ocr-assembly-[version].jar` or `secret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "license = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2MjMyMjM3NDcsImlhdCI6MTU5MTY4Nzc0NywidW5pcXVlX2lkIjoiZTc2NTZjNDItYWEyMi0xMWVhLTgyMjAtYTZjZmVjOWIxMDE1In0.MxySiRfPvcGRIGmHREMKc_3XB38W4k00iiJjt2UvAO9hcft4hetaJ1s_C6Xo94meA1u5uX4KPML6eOx1dCw9TXe4f2PK5mTorrgENM5LVZaB_P75EfYyGq06Yn3BYRQDnItE6SWkq-BLSYwgJNMmd6xpax2gVel8XILyBxjpGtuIISYZ8Q3YpHWSgF6dnO8RAvdLDQ3NupY1VvKH0-PhlRYz9Hq453xX3hbAPcn631FhN_bzvsmOdfU4qGGLdLlfk1802uQ7bf8Iy-vHyfvriRCdj906z25N9-FlLo1ng-yX2RhwQESyVXSDPKYicDhrbrqy6foaIPattx1nbC_Emw\"\n",
    "secret = \"\"\n",
    "version = secret.split(\"-\")[0]\n",
    "spark_ocr_jar_path = \"../../target/scala-2.11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if python -c 'import google.colab' &> /dev/null; then\n",
    "    echo \"Run on Google Colab!\"\n",
    "    echo \"Install Open JDK\"\n",
    "    apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "    java -version\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "  os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spark-nlp==2.4.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install from PYPI using secret\n",
    "%pip install spark-nlp==2.4.5\n",
    "#%pip install spark-ocr==$version --user --extra-index-url=https://pypi.johnsnowlabs.com/$secret --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing e:\\ideaproject\\spark-ocr-master\\python\\dist\\spark-ocr-1.5.0rc1.tar.gz\n",
      "Requirement already satisfied: numpy==1.17.4 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from spark-ocr==1.5.0rc1) (1.17.4)\n",
      "Requirement already satisfied: pillow==6.2.1 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from spark-ocr==1.5.0rc1) (6.2.1)\n",
      "Requirement already satisfied: py4j==0.10.7 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spark-ocr==1.5.0rc1) (0.10.7)\n",
      "Requirement already satisfied: pyspark==2.4.4 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spark-ocr==1.5.0rc1) (2.4.4)\n",
      "Requirement already satisfied: python-levenshtein==0.12.0 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from spark-ocr==1.5.0rc1) (0.12.0)\n",
      "Requirement already satisfied: scikit-image==0.16.2 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from spark-ocr==1.5.0rc1) (0.16.2)\n",
      "Requirement already satisfied: implicits==1.0.2 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from spark-ocr==1.5.0rc1) (1.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-levenshtein==0.12.0->spark-ocr==1.5.0rc1) (46.1.3)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from scikit-image==0.16.2->spark-ocr==1.5.0rc1) (1.1.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-image==0.16.2->spark-ocr==1.5.0rc1) (3.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from scikit-image==0.16.2->spark-ocr==1.5.0rc1) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-image==0.16.2->spark-ocr==1.5.0rc1) (1.4.1)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from scikit-image==0.16.2->spark-ocr==1.5.0rc1) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==1.5.0rc1) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==1.5.0rc1) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==1.5.0rc1) (2.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==1.5.0rc1) (1.1.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from networkx>=2.0->scikit-image==0.16.2->spark-ocr==1.5.0rc1) (4.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==1.5.0rc1) (1.12.0)\n",
      "Building wheels for collected packages: spark-ocr\n",
      "  Building wheel for spark-ocr (setup.py): started\n",
      "  Building wheel for spark-ocr (setup.py): finished with status 'done'\n",
      "  Created wheel for spark-ocr: filename=spark_ocr-1.5.0rc1-py3-none-any.whl size=6437168 sha256=4e229964470f5c0ebe78b4004800773cab30c01c85f7b444cae561e70583c74c\n",
      "  Stored in directory: c:\\users\\pc\\appdata\\local\\pip\\cache\\wheels\\24\\88\\1e\\f3a4a06a26170f07dd7dfeec093c6611712c1b51dd1547c039\n",
      "Successfully built spark-ocr\n",
      "Installing collected packages: spark-ocr\n",
      "  Attempting uninstall: spark-ocr\n",
      "    Found existing installation: spark-ocr 1.5.0rc1\n",
      "    Uninstalling spark-ocr-1.5.0rc1:\n",
      "      Successfully uninstalled spark-ocr-1.5.0rc1\n",
      "Successfully installed spark-ocr-1.5.0rc1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " %pip install --user ../../python/dist/spark-ocr-1.5.0rc1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of spark session\n",
    "Need specify path to `spark-ocr-assembly.jar` or `secret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark OCR</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e42a6c6cc8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparkocr import start\n",
    "\n",
    "if license:\n",
    "    os.environ['JSL_OCR_LICENSE'] = license\n",
    "\n",
    "spark = start(secret=secret, jar_path=spark_ocr_jar_path, nlp_version=\"2.4.5\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  col\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.base import *\n",
    "from sparkocr.transformers import *\n",
    "from sparkocr.enums import *\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define OCR transformers and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read binary as image\n",
    "binary_to_image = BinaryToImage()\n",
    "binary_to_image.setInputCol(\"content\")\n",
    "binary_to_image.setOutputCol(\"image\")\n",
    "\n",
    "# Binarize using adaptive tresholding\n",
    "binarizer = ImageAdaptiveThresholding()\n",
    "binarizer.setInputCol(\"image\")\n",
    "binarizer.setOutputCol(\"binarized_image\")\n",
    "\n",
    "# Apply morphology operation\n",
    "operation = ImageMorphologyOperation()\n",
    "operation.setKernelShape(KernelShape.SQUARE)\n",
    "operation.setKernelSize(2)\n",
    "operation.setInputCol(\"binarized_image\")\n",
    "operation.setOutputCol(\"opening_image\")\n",
    "\n",
    "# Remove small objects\n",
    "remove_objects = ImageRemoveObjects()\n",
    "remove_objects.setInputCol(\"opening_image\")\n",
    "remove_objects.setOutputCol(\"corrected_image\")\n",
    "remove_objects.setMinSizeFont(48)\n",
    "\n",
    "# Run tesseract OCR for corrected image\n",
    "ocr_corrected = ImageBrandsToText()\n",
    "ocr_corrected.setInputCol(\"corrected_image\")\n",
    "ocr_corrected.setOutputCol(\"image_brands\")\n",
    "ocr_corrected.setIgnoreResolution(False)\n",
    "ocr_corrected.setOcrParams([\"preserve_interword_spaces=1\", ])\n",
    "ocr_corrected.setPageSegMode(PageSegmentationMode.SINGLE_WORD)\n",
    "ocr_corrected.setBrandsCoords(\"\"\"\n",
    "              [\n",
    "                 {\n",
    "                    \"name\": \"name\",\n",
    "                    \"rectangle\": {\n",
    "                       \"x\": 250,\n",
    "                       \"y\": 158,\n",
    "                       \"width\": 204,\n",
    "                       \"height\": 23\n",
    "                    }\n",
    "                 },\n",
    "                 {\n",
    "                    \"name\": \"issue_date\",\n",
    "                    \"rectangle\": {\n",
    "                       \"x\": 641,\n",
    "                       \"y\": 156,\n",
    "                       \"width\": 129,\n",
    "                       \"height\": 20\n",
    "                    }\n",
    "                 },\n",
    "                 {\n",
    "                    \"name\": \"serial_number\",\n",
    "                    \"rectangle\": {\n",
    "                       \"x\": 570,\n",
    "                       \"y\": 343,\n",
    "                       \"width\": 188,\n",
    "                       \"height\": 33\n",
    "                    }\n",
    "                 }\n",
    "              ]\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# OCR pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    binary_to_image,\n",
    "    binarizer,\n",
    "    operation,\n",
    "    remove_objects,\n",
    "    ocr_corrected\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Image  as binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_path = '././data/images/dollar_bonds/*.jpg'\n",
    "image_df = spark.read.format(\"binaryFile\").load(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run OCR pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ocr_result = pipeline.fit(image_df).transform(image_df).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o273.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 4, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\Spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 362, in main\n  File \"E:\\Spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 722, in read_int\n    length = stream.read(4)\n  File \"c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:125)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\r\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\r\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\Spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 362, in main\n  File \"E:\\Spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 722, in read_int\n    length = stream.read(4)\n  File \"c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:125)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\r\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\r\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-c6c1a2baa95e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m results = ocr_result.select(col(\"path\"),col(\"image_brands.name.text\").alias(\"name\"),col(\"image_brands.issue_date.text\").alias(\"issue_date\") \\\n\u001B[1;32m----> 2\u001B[1;33m                         ,col(\"image_brands.serial_number.text\").alias(\"serial_number\")).collect()\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprintln\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001B[0m in \u001B[0;36mcollect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    532\u001B[0m         \"\"\"\n\u001B[0;32m    533\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 534\u001B[1;33m             \u001B[0msock_info\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollectToPython\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    535\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mBatchedSerializer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mPickleSerializer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1255\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1256\u001B[0m         return_value = get_return_value(\n\u001B[1;32m-> 1257\u001B[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[0;32m   1258\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1259\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m     61\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m             \u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoString\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 328\u001B[1;33m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[0;32m    329\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o273.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 4, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\Spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 362, in main\n  File \"E:\\Spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 722, in read_int\n    length = stream.read(4)\n  File \"c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:125)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\r\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\r\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\Spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 362, in main\n  File \"E:\\Spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 722, in read_int\n    length = stream.read(4)\n  File \"c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:125)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\r\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\r\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "results = ocr_result.select(col(\"path\"),col(\"image_brands.name.text\").alias(\"name\"),col(\"image_brands.issue_date.text\").alias(\"issue_date\") \\\n",
    "                        ,col(\"image_brands.serial_number.text\").alias(\"serial_number\")).collect()\n",
    "for row in results:\n",
    "    print(colored(\"path:\\n%s\" % row.path, \"red\"))\n",
    "    print(\"Name:\\n%s\" % row.name)\n",
    "    print(\"Issue Date:\\n%s\" % row.issue_date)\n",
    "    print(\"Serial Number:\\n%s\" % row.serial_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}